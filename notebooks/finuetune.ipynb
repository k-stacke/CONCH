{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install info-nce-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from info_nce import InfoNCE \n",
    "from tqdm import tqdm\n",
    "\n",
    "from conch.open_clip_custom import create_model_from_pretrained, get_tokenizer, tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model \"create_model_from_pretrained\"\n",
    "TODO: Double check that image size of the conch is as expected (it runs with 224px images, but is it ok?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/src/CONCH/conch/open_clip_custom/factory.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=map_location)\n"
     ]
    }
   ],
   "source": [
    "model_cfg = 'conch_ViT-B-16'\n",
    "checkpoint_path = './checkpoints/CONCH/pytorch_model.bin'\n",
    "model, preprocess = create_model_from_pretrained(model_cfg, checkpoint_path)\n",
    "\n",
    "# _ = model.eval()\n",
    "\n",
    "# We are only interensted in the ViT part of the model\n",
    "# Since the config states that attentional_pool_caption is true, then the default forward function does not use the head, nor normalization\n",
    "# TODO: Check if head and l2 normalization are used in finetuning\n",
    "model_vit = model.visual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisualModel(\n",
      "  (trunk): VisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "    (patch_drop): Identity()\n",
      "    (norm_pre): Identity()\n",
      "    (blocks): Sequential(\n",
      "      (0): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (1): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (2): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (3): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (4): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (5): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (6): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (7): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (8): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (9): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (10): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (11): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (fc_norm): Identity()\n",
      "    (head_drop): Dropout(p=0.0, inplace=False)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (attn_pool_contrast): AttentionalPooler(\n",
      "    (attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (ln_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (ln_k): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (ln_contrast): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (head): Sequential()\n",
      "  (attn_pool_caption): AttentionalPooler(\n",
      "    (attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (ln_q): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (ln_k): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (ln_caption): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_vit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers\n",
    "for param in model_vit.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the last 3 layer\n",
    "for param in model_vit.trunk.blocks[-3:].parameters():\n",
    "    param.requires_grad = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: proj_contrast | Trainable: False\n",
      "Layer: trunk.cls_token | Trainable: False\n",
      "Layer: trunk.pos_embed | Trainable: False\n",
      "Layer: trunk.patch_embed.proj.weight | Trainable: False\n",
      "Layer: trunk.patch_embed.proj.bias | Trainable: False\n",
      "Layer: trunk.blocks.0.norm1.weight | Trainable: False\n",
      "Layer: trunk.blocks.0.norm1.bias | Trainable: False\n",
      "Layer: trunk.blocks.0.attn.qkv.weight | Trainable: False\n",
      "Layer: trunk.blocks.0.attn.qkv.bias | Trainable: False\n",
      "Layer: trunk.blocks.0.attn.proj.weight | Trainable: False\n",
      "Layer: trunk.blocks.0.attn.proj.bias | Trainable: False\n",
      "Layer: trunk.blocks.0.norm2.weight | Trainable: False\n",
      "Layer: trunk.blocks.0.norm2.bias | Trainable: False\n",
      "Layer: trunk.blocks.0.mlp.fc1.weight | Trainable: False\n",
      "Layer: trunk.blocks.0.mlp.fc1.bias | Trainable: False\n",
      "Layer: trunk.blocks.0.mlp.fc2.weight | Trainable: False\n",
      "Layer: trunk.blocks.0.mlp.fc2.bias | Trainable: False\n",
      "Layer: trunk.blocks.1.norm1.weight | Trainable: False\n",
      "Layer: trunk.blocks.1.norm1.bias | Trainable: False\n",
      "Layer: trunk.blocks.1.attn.qkv.weight | Trainable: False\n",
      "Layer: trunk.blocks.1.attn.qkv.bias | Trainable: False\n",
      "Layer: trunk.blocks.1.attn.proj.weight | Trainable: False\n",
      "Layer: trunk.blocks.1.attn.proj.bias | Trainable: False\n",
      "Layer: trunk.blocks.1.norm2.weight | Trainable: False\n",
      "Layer: trunk.blocks.1.norm2.bias | Trainable: False\n",
      "Layer: trunk.blocks.1.mlp.fc1.weight | Trainable: False\n",
      "Layer: trunk.blocks.1.mlp.fc1.bias | Trainable: False\n",
      "Layer: trunk.blocks.1.mlp.fc2.weight | Trainable: False\n",
      "Layer: trunk.blocks.1.mlp.fc2.bias | Trainable: False\n",
      "Layer: trunk.blocks.2.norm1.weight | Trainable: False\n",
      "Layer: trunk.blocks.2.norm1.bias | Trainable: False\n",
      "Layer: trunk.blocks.2.attn.qkv.weight | Trainable: False\n",
      "Layer: trunk.blocks.2.attn.qkv.bias | Trainable: False\n",
      "Layer: trunk.blocks.2.attn.proj.weight | Trainable: False\n",
      "Layer: trunk.blocks.2.attn.proj.bias | Trainable: False\n",
      "Layer: trunk.blocks.2.norm2.weight | Trainable: False\n",
      "Layer: trunk.blocks.2.norm2.bias | Trainable: False\n",
      "Layer: trunk.blocks.2.mlp.fc1.weight | Trainable: False\n",
      "Layer: trunk.blocks.2.mlp.fc1.bias | Trainable: False\n",
      "Layer: trunk.blocks.2.mlp.fc2.weight | Trainable: False\n",
      "Layer: trunk.blocks.2.mlp.fc2.bias | Trainable: False\n",
      "Layer: trunk.blocks.3.norm1.weight | Trainable: False\n",
      "Layer: trunk.blocks.3.norm1.bias | Trainable: False\n",
      "Layer: trunk.blocks.3.attn.qkv.weight | Trainable: False\n",
      "Layer: trunk.blocks.3.attn.qkv.bias | Trainable: False\n",
      "Layer: trunk.blocks.3.attn.proj.weight | Trainable: False\n",
      "Layer: trunk.blocks.3.attn.proj.bias | Trainable: False\n",
      "Layer: trunk.blocks.3.norm2.weight | Trainable: False\n",
      "Layer: trunk.blocks.3.norm2.bias | Trainable: False\n",
      "Layer: trunk.blocks.3.mlp.fc1.weight | Trainable: False\n",
      "Layer: trunk.blocks.3.mlp.fc1.bias | Trainable: False\n",
      "Layer: trunk.blocks.3.mlp.fc2.weight | Trainable: False\n",
      "Layer: trunk.blocks.3.mlp.fc2.bias | Trainable: False\n",
      "Layer: trunk.blocks.4.norm1.weight | Trainable: False\n",
      "Layer: trunk.blocks.4.norm1.bias | Trainable: False\n",
      "Layer: trunk.blocks.4.attn.qkv.weight | Trainable: False\n",
      "Layer: trunk.blocks.4.attn.qkv.bias | Trainable: False\n",
      "Layer: trunk.blocks.4.attn.proj.weight | Trainable: False\n",
      "Layer: trunk.blocks.4.attn.proj.bias | Trainable: False\n",
      "Layer: trunk.blocks.4.norm2.weight | Trainable: False\n",
      "Layer: trunk.blocks.4.norm2.bias | Trainable: False\n",
      "Layer: trunk.blocks.4.mlp.fc1.weight | Trainable: False\n",
      "Layer: trunk.blocks.4.mlp.fc1.bias | Trainable: False\n",
      "Layer: trunk.blocks.4.mlp.fc2.weight | Trainable: False\n",
      "Layer: trunk.blocks.4.mlp.fc2.bias | Trainable: False\n",
      "Layer: trunk.blocks.5.norm1.weight | Trainable: False\n",
      "Layer: trunk.blocks.5.norm1.bias | Trainable: False\n",
      "Layer: trunk.blocks.5.attn.qkv.weight | Trainable: False\n",
      "Layer: trunk.blocks.5.attn.qkv.bias | Trainable: False\n",
      "Layer: trunk.blocks.5.attn.proj.weight | Trainable: False\n",
      "Layer: trunk.blocks.5.attn.proj.bias | Trainable: False\n",
      "Layer: trunk.blocks.5.norm2.weight | Trainable: False\n",
      "Layer: trunk.blocks.5.norm2.bias | Trainable: False\n",
      "Layer: trunk.blocks.5.mlp.fc1.weight | Trainable: False\n",
      "Layer: trunk.blocks.5.mlp.fc1.bias | Trainable: False\n",
      "Layer: trunk.blocks.5.mlp.fc2.weight | Trainable: False\n",
      "Layer: trunk.blocks.5.mlp.fc2.bias | Trainable: False\n",
      "Layer: trunk.blocks.6.norm1.weight | Trainable: False\n",
      "Layer: trunk.blocks.6.norm1.bias | Trainable: False\n",
      "Layer: trunk.blocks.6.attn.qkv.weight | Trainable: False\n",
      "Layer: trunk.blocks.6.attn.qkv.bias | Trainable: False\n",
      "Layer: trunk.blocks.6.attn.proj.weight | Trainable: False\n",
      "Layer: trunk.blocks.6.attn.proj.bias | Trainable: False\n",
      "Layer: trunk.blocks.6.norm2.weight | Trainable: False\n",
      "Layer: trunk.blocks.6.norm2.bias | Trainable: False\n",
      "Layer: trunk.blocks.6.mlp.fc1.weight | Trainable: False\n",
      "Layer: trunk.blocks.6.mlp.fc1.bias | Trainable: False\n",
      "Layer: trunk.blocks.6.mlp.fc2.weight | Trainable: False\n",
      "Layer: trunk.blocks.6.mlp.fc2.bias | Trainable: False\n",
      "Layer: trunk.blocks.7.norm1.weight | Trainable: False\n",
      "Layer: trunk.blocks.7.norm1.bias | Trainable: False\n",
      "Layer: trunk.blocks.7.attn.qkv.weight | Trainable: False\n",
      "Layer: trunk.blocks.7.attn.qkv.bias | Trainable: False\n",
      "Layer: trunk.blocks.7.attn.proj.weight | Trainable: False\n",
      "Layer: trunk.blocks.7.attn.proj.bias | Trainable: False\n",
      "Layer: trunk.blocks.7.norm2.weight | Trainable: False\n",
      "Layer: trunk.blocks.7.norm2.bias | Trainable: False\n",
      "Layer: trunk.blocks.7.mlp.fc1.weight | Trainable: False\n",
      "Layer: trunk.blocks.7.mlp.fc1.bias | Trainable: False\n",
      "Layer: trunk.blocks.7.mlp.fc2.weight | Trainable: False\n",
      "Layer: trunk.blocks.7.mlp.fc2.bias | Trainable: False\n",
      "Layer: trunk.blocks.8.norm1.weight | Trainable: False\n",
      "Layer: trunk.blocks.8.norm1.bias | Trainable: False\n",
      "Layer: trunk.blocks.8.attn.qkv.weight | Trainable: False\n",
      "Layer: trunk.blocks.8.attn.qkv.bias | Trainable: False\n",
      "Layer: trunk.blocks.8.attn.proj.weight | Trainable: False\n",
      "Layer: trunk.blocks.8.attn.proj.bias | Trainable: False\n",
      "Layer: trunk.blocks.8.norm2.weight | Trainable: False\n",
      "Layer: trunk.blocks.8.norm2.bias | Trainable: False\n",
      "Layer: trunk.blocks.8.mlp.fc1.weight | Trainable: False\n",
      "Layer: trunk.blocks.8.mlp.fc1.bias | Trainable: False\n",
      "Layer: trunk.blocks.8.mlp.fc2.weight | Trainable: False\n",
      "Layer: trunk.blocks.8.mlp.fc2.bias | Trainable: False\n",
      "Layer: trunk.blocks.9.norm1.weight | Trainable: True\n",
      "Layer: trunk.blocks.9.norm1.bias | Trainable: True\n",
      "Layer: trunk.blocks.9.attn.qkv.weight | Trainable: True\n",
      "Layer: trunk.blocks.9.attn.qkv.bias | Trainable: True\n",
      "Layer: trunk.blocks.9.attn.proj.weight | Trainable: True\n",
      "Layer: trunk.blocks.9.attn.proj.bias | Trainable: True\n",
      "Layer: trunk.blocks.9.norm2.weight | Trainable: True\n",
      "Layer: trunk.blocks.9.norm2.bias | Trainable: True\n",
      "Layer: trunk.blocks.9.mlp.fc1.weight | Trainable: True\n",
      "Layer: trunk.blocks.9.mlp.fc1.bias | Trainable: True\n",
      "Layer: trunk.blocks.9.mlp.fc2.weight | Trainable: True\n",
      "Layer: trunk.blocks.9.mlp.fc2.bias | Trainable: True\n",
      "Layer: trunk.blocks.10.norm1.weight | Trainable: True\n",
      "Layer: trunk.blocks.10.norm1.bias | Trainable: True\n",
      "Layer: trunk.blocks.10.attn.qkv.weight | Trainable: True\n",
      "Layer: trunk.blocks.10.attn.qkv.bias | Trainable: True\n",
      "Layer: trunk.blocks.10.attn.proj.weight | Trainable: True\n",
      "Layer: trunk.blocks.10.attn.proj.bias | Trainable: True\n",
      "Layer: trunk.blocks.10.norm2.weight | Trainable: True\n",
      "Layer: trunk.blocks.10.norm2.bias | Trainable: True\n",
      "Layer: trunk.blocks.10.mlp.fc1.weight | Trainable: True\n",
      "Layer: trunk.blocks.10.mlp.fc1.bias | Trainable: True\n",
      "Layer: trunk.blocks.10.mlp.fc2.weight | Trainable: True\n",
      "Layer: trunk.blocks.10.mlp.fc2.bias | Trainable: True\n",
      "Layer: trunk.blocks.11.norm1.weight | Trainable: True\n",
      "Layer: trunk.blocks.11.norm1.bias | Trainable: True\n",
      "Layer: trunk.blocks.11.attn.qkv.weight | Trainable: True\n",
      "Layer: trunk.blocks.11.attn.qkv.bias | Trainable: True\n",
      "Layer: trunk.blocks.11.attn.proj.weight | Trainable: True\n",
      "Layer: trunk.blocks.11.attn.proj.bias | Trainable: True\n",
      "Layer: trunk.blocks.11.norm2.weight | Trainable: True\n",
      "Layer: trunk.blocks.11.norm2.bias | Trainable: True\n",
      "Layer: trunk.blocks.11.mlp.fc1.weight | Trainable: True\n",
      "Layer: trunk.blocks.11.mlp.fc1.bias | Trainable: True\n",
      "Layer: trunk.blocks.11.mlp.fc2.weight | Trainable: True\n",
      "Layer: trunk.blocks.11.mlp.fc2.bias | Trainable: True\n",
      "Layer: trunk.norm.weight | Trainable: False\n",
      "Layer: trunk.norm.bias | Trainable: False\n",
      "Layer: attn_pool_contrast.query | Trainable: False\n",
      "Layer: attn_pool_contrast.attn.q_proj_weight | Trainable: False\n",
      "Layer: attn_pool_contrast.attn.k_proj_weight | Trainable: False\n",
      "Layer: attn_pool_contrast.attn.v_proj_weight | Trainable: False\n",
      "Layer: attn_pool_contrast.attn.in_proj_bias | Trainable: False\n",
      "Layer: attn_pool_contrast.attn.out_proj.weight | Trainable: False\n",
      "Layer: attn_pool_contrast.attn.out_proj.bias | Trainable: False\n",
      "Layer: attn_pool_contrast.ln_q.weight | Trainable: False\n",
      "Layer: attn_pool_contrast.ln_q.bias | Trainable: False\n",
      "Layer: attn_pool_contrast.ln_k.weight | Trainable: False\n",
      "Layer: attn_pool_contrast.ln_k.bias | Trainable: False\n",
      "Layer: ln_contrast.weight | Trainable: False\n",
      "Layer: ln_contrast.bias | Trainable: False\n",
      "Layer: attn_pool_caption.query | Trainable: False\n",
      "Layer: attn_pool_caption.attn.in_proj_weight | Trainable: False\n",
      "Layer: attn_pool_caption.attn.in_proj_bias | Trainable: False\n",
      "Layer: attn_pool_caption.attn.out_proj.weight | Trainable: False\n",
      "Layer: attn_pool_caption.attn.out_proj.bias | Trainable: False\n",
      "Layer: attn_pool_caption.ln_q.weight | Trainable: False\n",
      "Layer: attn_pool_caption.ln_q.bias | Trainable: False\n",
      "Layer: attn_pool_caption.ln_k.weight | Trainable: False\n",
      "Layer: attn_pool_caption.ln_k.bias | Trainable: False\n",
      "Layer: ln_caption.weight | Trainable: False\n",
      "Layer: ln_caption.bias | Trainable: False\n"
     ]
    }
   ],
   "source": [
    "for name, param in model_vit.named_parameters():\n",
    "    print(f\"Layer: {name} | Trainable: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 3-layer MLP\n",
    "st_raw_count_dims = 280\n",
    "output_dims = 512\n",
    "\n",
    "model_mlp = torch.nn.Sequential(\n",
    "    torch.nn.Linear(st_raw_count_dims, output_dims*2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(output_dims*2, output_dims*2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(output_dims*2, output_dims)\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytroch dataloader for images only\n",
    "import h5py\n",
    "import scanpy as sc\n",
    "\n",
    "class ImageExpressionDataset(Dataset):\n",
    "    def __init__(self, cases, image_dir, expression_dir, selected_genes=None, transform=None):\n",
    "        self.cases = cases\n",
    "        self.image_dir = image_dir\n",
    "        self.expression_dir = expression_dir\n",
    "        self.transform = transform\n",
    "        self.selected_genes = selected_genes\n",
    "        \n",
    "        self.image_size = 224\n",
    "        \n",
    "        # TODO: This is not the optimal way to load the data, \n",
    "        # as it loads all the data in memory, and is very slow at start up\n",
    "\n",
    "        # From dataset, read what cases to load\n",
    "        # Load cases as anndata\n",
    "        # Filter genes that are included from list\n",
    "        self.data_df = self.load_data()\n",
    "        print(f'Loaded {len(self.data_df)} patches with expression data')\n",
    "\n",
    "\n",
    "    def load_data(self):\n",
    "        # For each case, load the expression data and the patches\n",
    "        dfs = []\n",
    "        for case in self.cases:\n",
    "            # Load the patches\n",
    "            df_patches = self.load_patches(case)\n",
    "            # Load the expression\n",
    "            adata = self.load_expressions(case)\n",
    "            # Merge the data\n",
    "            df_patches.loc[:, 'expression'] = list(adata[df_patches.barcode, :].X)\n",
    "            \n",
    "            dfs.append(df_patches)\n",
    "\n",
    "        return pd.concat(dfs)\n",
    "\n",
    "    def load_patches(self, case):\n",
    "        # Open the file in read mode\n",
    "        with h5py.File(f'{self.image_dir}/{case}.h5', 'r') as file:\n",
    "            # Get the data\n",
    "            imgs = list(file['img'])\n",
    "            barcodes = list(file['barcode'])\n",
    "            coords = list(file['coords'])\n",
    "\n",
    "        barcodes = [b[0].decode('utf-8') for b in barcodes]\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                'barcode': barcodes,\n",
    "                'coord': coords,\n",
    "                'img': [Image.fromarray(im) for im in imgs]\n",
    "            }\n",
    "        )\n",
    "        return df\n",
    "\n",
    "\n",
    "    def load_expressions(self, case):\n",
    "       \n",
    "        adata = sc.read_h5ad(f'{self.expression_dir}/{case}.h5ad')\n",
    "        adata.obs['batch'] = case  # Add filename as batch key\n",
    "\n",
    "        # Filter to include only selected genes\n",
    "        if self.selected_genes is not None:\n",
    "            adata = adata[:, self.selected_genes]\n",
    "\n",
    "        # Normalize and log transform\n",
    "        # TODO: Should this be done for the whole dataset or per case?\n",
    "        sc.pp.normalize_total(adata)\n",
    "        # Log transformation\n",
    "        sc.pp.log1p(adata)\n",
    "        return adata\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df.index)\n",
    "   \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data_df.iloc[index]\n",
    "        \n",
    "        image = row.img\n",
    "        expression = row.expression\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = transforms.ToTensor()(image)\n",
    "\n",
    "        return image, expression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kstacke/miniconda3/envs/conch/lib/python3.10/site-packages/scanpy/preprocessing/_normalization.py:207: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n",
      "/home/kstacke/miniconda3/envs/conch/lib/python3.10/site-packages/scanpy/preprocessing/_normalization.py:207: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n",
      "/home/kstacke/miniconda3/envs/conch/lib/python3.10/site-packages/scanpy/preprocessing/_normalization.py:234: UserWarning: Some cells have zero counts\n",
      "  warn(UserWarning(\"Some cells have zero counts\"))\n",
      "/home/kstacke/miniconda3/envs/conch/lib/python3.10/site-packages/scanpy/preprocessing/_normalization.py:207: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n",
      "/home/kstacke/miniconda3/envs/conch/lib/python3.10/site-packages/scanpy/preprocessing/_normalization.py:234: UserWarning: Some cells have zero counts\n",
      "  warn(UserWarning(\"Some cells have zero counts\"))\n",
      "/home/kstacke/miniconda3/envs/conch/lib/python3.10/site-packages/scanpy/preprocessing/_normalization.py:207: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n",
      "/home/kstacke/miniconda3/envs/conch/lib/python3.10/site-packages/scanpy/preprocessing/_normalization.py:234: UserWarning: Some cells have zero counts\n",
      "  warn(UserWarning(\"Some cells have zero counts\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 41893 patches with expression data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kstacke/miniconda3/envs/conch/lib/python3.10/site-packages/scanpy/preprocessing/_normalization.py:207: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n",
      "/home/kstacke/miniconda3/envs/conch/lib/python3.10/site-packages/scanpy/preprocessing/_normalization.py:234: UserWarning: Some cells have zero counts\n",
      "  warn(UserWarning(\"Some cells have zero counts\"))\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of your dataset\n",
    "import json\n",
    "aug = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "cases = ['TENX99', 'TENX96', 'TENX95', 'NCBI783', 'NCBI785']\n",
    "expression_dir = '/mnt/c/data/hest_data/st'\n",
    "image_dir = '/mnt/c/data/hest_data/patches'\n",
    "selected_genes = json.load(open('/mnt/c/data/hest_data/filtered_genes.json'))\n",
    "\n",
    "dataset = ImageExpressionDataset(cases=cases, image_dir=image_dir, expression_dir=expression_dir,\n",
    "                                 selected_genes=selected_genes, transform=aug)\n",
    "\n",
    "# # Create a PyTorch DataLoader\n",
    "batch_size = 32 # TODO: change to 1024\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n",
      "torch.Size([32, 280])\n"
     ]
    }
   ],
   "source": [
    "# Test dataloader\n",
    "for image, expression in data_loader:\n",
    "    print(image.shape)\n",
    "    print(expression.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add layerwise learning rate decay rate of 0.7 for ViT model\n",
    "\n",
    "\n",
    "# Set up the optimizer to only optimize the paramters that are trainable\n",
    "optimizer_vit = torch.optim.Adam(filter(lambda p: p.requires_grad, model_vit.parameters()), lr=0.0004)\n",
    "# MLP optimizer\n",
    "optimizer_mlp = torch.optim.Adam(model_mlp.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "scheduler_vit = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_vit, T_max=epochs)\n",
    "scheduler_mlp = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_mlp, T_max=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.02 # according to the paper\n",
    "# info-nce-pytorch\n",
    "infoloss = InfoNCE(temperature=temperature, negative_mode='unpaired')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=280, out_features=1024, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=1024, out_features=512, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_vit.cuda()\n",
    "model_mlp.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: [1/10] Loss: 1.5831:  14%|█████▉                                    | 186/1310 [01:03<06:26,  2.91it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m optimizer_mlp\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     19\u001b[0m total_num \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size\n\u001b[0;32m---> 20\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m batch_size\n\u001b[1;32m     21\u001b[0m train_bar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Epoch: [\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m] Loss: \u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, epochs, total_loss \u001b[38;5;241m/\u001b[39m total_num))\n\u001b[1;32m     23\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m total_num\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "\n",
    "    model_vit.train()\n",
    "    model_mlp.train()\n",
    "\n",
    "    total_loss, total_num, train_bar = 0.0, 0, tqdm(data_loader)\n",
    "    for img, expr in train_bar:\n",
    "        img, expr = img.cuda(non_blocking=True), expr.cuda(non_blocking=True)\n",
    "        out_1, _ = model_vit(img)\n",
    "        out_2 = model_mlp(expr)\n",
    "        \n",
    "        loss = infoloss(out_1, out_2)\n",
    "        optimizer_vit.zero_grad()\n",
    "        optimizer_mlp.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_vit.step()\n",
    "        optimizer_mlp.step()\n",
    "\n",
    "        total_num += batch_size\n",
    "        total_loss += loss.item() * batch_size\n",
    "        train_bar.set_description('Train Epoch: [{}/{}] Loss: {:.4f}'.format(epoch, epochs, total_loss / total_num))\n",
    "\n",
    "        train_loss = total_loss / total_num\n",
    "    \n",
    "    scheduler_vit.step()\n",
    "    scheduler_mlp.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
