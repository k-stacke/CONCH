{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install info-nce-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hu-eki/miniconda3/envs/conch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/hu-eki/miniconda3/envs/conch/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from info_nce import InfoNCE \n",
    "from tqdm import tqdm\n",
    "\n",
    "from conch.open_clip_custom import create_model_from_pretrained, get_tokenizer, tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model \"create_model_from_pretrained\"\n",
    "TODO: Double check that image size of the conch is as expected (it runs with 224px images, but is it ok?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "local_weights = False\n",
    "model_cfg = 'conch_ViT-B-16'\n",
    "\n",
    "if local_weights:\n",
    "    checkpoint_path = './checkpoints/CONCH/pytorch_model.bin'\n",
    "else:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()  # take environment variables from .env.\n",
    "    checkpoint_path = 'hf_hub:MahmoodLab/conch'\n",
    "    hf_auth_token = os.getenv(\"HF_AUTH_TOKEN\")\n",
    "    if hf_auth_token is None:\n",
    "        raise ValueError(\"HF_AUTH_TOKEN environment variable not set\")\n",
    "model, preprocess = create_model_from_pretrained(model_cfg, checkpoint_path, hf_auth_token=hf_auth_token)\n",
    "\n",
    "# _ = model.eval()\n",
    "\n",
    "# We are only interensted in the ViT part of the model\n",
    "# Since the config states that attentional_pool_caption is true, then the default forward function does not use the head, nor normalization\n",
    "# TODO: Check if head and l2 normalization are used in finetuning\n",
    "model_vit = model.visual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisualModel(\n",
      "  (trunk): VisionTransformer(\n",
      "    (patch_embed): PatchEmbed(\n",
      "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "      (norm): Identity()\n",
      "    )\n",
      "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "    (patch_drop): Identity()\n",
      "    (norm_pre): Identity()\n",
      "    (blocks): Sequential(\n",
      "      (0): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (1): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (2): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (3): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (4): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (5): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (6): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (7): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (8): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (9): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (10): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "      (11): Block(\n",
      "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (attn): Attention(\n",
      "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (q_norm): Identity()\n",
      "          (k_norm): Identity()\n",
      "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls1): Identity()\n",
      "        (drop_path1): Identity()\n",
      "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (drop1): Dropout(p=0.0, inplace=False)\n",
      "          (norm): Identity()\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (drop2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ls2): Identity()\n",
      "        (drop_path2): Identity()\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "    (fc_norm): Identity()\n",
      "    (head_drop): Dropout(p=0.0, inplace=False)\n",
      "    (head): Identity()\n",
      "  )\n",
      "  (attn_pool_contrast): AttentionalPooler(\n",
      "    (attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (ln_q): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (ln_k): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (ln_contrast): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (head): Sequential()\n",
      "  (attn_pool_caption): AttentionalPooler(\n",
      "    (attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "    )\n",
      "    (ln_q): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (ln_k): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (ln_caption): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_vit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers\n",
    "for param in model_vit.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the last 3 layer\n",
    "for param in model_vit.trunk.blocks[-3:].parameters():\n",
    "    param.requires_grad = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: proj_contrast | Trainable: False\n",
      "Layer: trunk.cls_token | Trainable: False\n",
      "Layer: trunk.pos_embed | Trainable: False\n",
      "Layer: trunk.patch_embed.proj.weight | Trainable: False\n",
      "Layer: trunk.patch_embed.proj.bias | Trainable: False\n",
      "Layer: trunk.blocks.0.norm1.weight | Trainable: False\n",
      "Layer: trunk.blocks.0.norm1.bias | Trainable: False\n",
      "Layer: trunk.blocks.0.attn.qkv.weight | Trainable: False\n",
      "Layer: trunk.blocks.0.attn.qkv.bias | Trainable: False\n",
      "Layer: trunk.blocks.0.attn.proj.weight | Trainable: False\n",
      "Layer: trunk.blocks.0.attn.proj.bias | Trainable: False\n",
      "Layer: trunk.blocks.0.norm2.weight | Trainable: False\n",
      "Layer: trunk.blocks.0.norm2.bias | Trainable: False\n",
      "Layer: trunk.blocks.0.mlp.fc1.weight | Trainable: False\n",
      "Layer: trunk.blocks.0.mlp.fc1.bias | Trainable: False\n",
      "Layer: trunk.blocks.0.mlp.fc2.weight | Trainable: False\n",
      "Layer: trunk.blocks.0.mlp.fc2.bias | Trainable: False\n",
      "Layer: trunk.blocks.1.norm1.weight | Trainable: False\n",
      "Layer: trunk.blocks.1.norm1.bias | Trainable: False\n",
      "Layer: trunk.blocks.1.attn.qkv.weight | Trainable: False\n",
      "Layer: trunk.blocks.1.attn.qkv.bias | Trainable: False\n",
      "Layer: trunk.blocks.1.attn.proj.weight | Trainable: False\n",
      "Layer: trunk.blocks.1.attn.proj.bias | Trainable: False\n",
      "Layer: trunk.blocks.1.norm2.weight | Trainable: False\n",
      "Layer: trunk.blocks.1.norm2.bias | Trainable: False\n",
      "Layer: trunk.blocks.1.mlp.fc1.weight | Trainable: False\n",
      "Layer: trunk.blocks.1.mlp.fc1.bias | Trainable: False\n",
      "Layer: trunk.blocks.1.mlp.fc2.weight | Trainable: False\n",
      "Layer: trunk.blocks.1.mlp.fc2.bias | Trainable: False\n",
      "Layer: trunk.blocks.2.norm1.weight | Trainable: False\n",
      "Layer: trunk.blocks.2.norm1.bias | Trainable: False\n",
      "Layer: trunk.blocks.2.attn.qkv.weight | Trainable: False\n",
      "Layer: trunk.blocks.2.attn.qkv.bias | Trainable: False\n",
      "Layer: trunk.blocks.2.attn.proj.weight | Trainable: False\n",
      "Layer: trunk.blocks.2.attn.proj.bias | Trainable: False\n",
      "Layer: trunk.blocks.2.norm2.weight | Trainable: False\n",
      "Layer: trunk.blocks.2.norm2.bias | Trainable: False\n",
      "Layer: trunk.blocks.2.mlp.fc1.weight | Trainable: False\n",
      "Layer: trunk.blocks.2.mlp.fc1.bias | Trainable: False\n",
      "Layer: trunk.blocks.2.mlp.fc2.weight | Trainable: False\n",
      "Layer: trunk.blocks.2.mlp.fc2.bias | Trainable: False\n",
      "Layer: trunk.blocks.3.norm1.weight | Trainable: False\n",
      "Layer: trunk.blocks.3.norm1.bias | Trainable: False\n",
      "Layer: trunk.blocks.3.attn.qkv.weight | Trainable: False\n",
      "Layer: trunk.blocks.3.attn.qkv.bias | Trainable: False\n",
      "Layer: trunk.blocks.3.attn.proj.weight | Trainable: False\n",
      "Layer: trunk.blocks.3.attn.proj.bias | Trainable: False\n",
      "Layer: trunk.blocks.3.norm2.weight | Trainable: False\n",
      "Layer: trunk.blocks.3.norm2.bias | Trainable: False\n",
      "Layer: trunk.blocks.3.mlp.fc1.weight | Trainable: False\n",
      "Layer: trunk.blocks.3.mlp.fc1.bias | Trainable: False\n",
      "Layer: trunk.blocks.3.mlp.fc2.weight | Trainable: False\n",
      "Layer: trunk.blocks.3.mlp.fc2.bias | Trainable: False\n",
      "Layer: trunk.blocks.4.norm1.weight | Trainable: False\n",
      "Layer: trunk.blocks.4.norm1.bias | Trainable: False\n",
      "Layer: trunk.blocks.4.attn.qkv.weight | Trainable: False\n",
      "Layer: trunk.blocks.4.attn.qkv.bias | Trainable: False\n",
      "Layer: trunk.blocks.4.attn.proj.weight | Trainable: False\n",
      "Layer: trunk.blocks.4.attn.proj.bias | Trainable: False\n",
      "Layer: trunk.blocks.4.norm2.weight | Trainable: False\n",
      "Layer: trunk.blocks.4.norm2.bias | Trainable: False\n",
      "Layer: trunk.blocks.4.mlp.fc1.weight | Trainable: False\n",
      "Layer: trunk.blocks.4.mlp.fc1.bias | Trainable: False\n",
      "Layer: trunk.blocks.4.mlp.fc2.weight | Trainable: False\n",
      "Layer: trunk.blocks.4.mlp.fc2.bias | Trainable: False\n",
      "Layer: trunk.blocks.5.norm1.weight | Trainable: False\n",
      "Layer: trunk.blocks.5.norm1.bias | Trainable: False\n",
      "Layer: trunk.blocks.5.attn.qkv.weight | Trainable: False\n",
      "Layer: trunk.blocks.5.attn.qkv.bias | Trainable: False\n",
      "Layer: trunk.blocks.5.attn.proj.weight | Trainable: False\n",
      "Layer: trunk.blocks.5.attn.proj.bias | Trainable: False\n",
      "Layer: trunk.blocks.5.norm2.weight | Trainable: False\n",
      "Layer: trunk.blocks.5.norm2.bias | Trainable: False\n",
      "Layer: trunk.blocks.5.mlp.fc1.weight | Trainable: False\n",
      "Layer: trunk.blocks.5.mlp.fc1.bias | Trainable: False\n",
      "Layer: trunk.blocks.5.mlp.fc2.weight | Trainable: False\n",
      "Layer: trunk.blocks.5.mlp.fc2.bias | Trainable: False\n",
      "Layer: trunk.blocks.6.norm1.weight | Trainable: False\n",
      "Layer: trunk.blocks.6.norm1.bias | Trainable: False\n",
      "Layer: trunk.blocks.6.attn.qkv.weight | Trainable: False\n",
      "Layer: trunk.blocks.6.attn.qkv.bias | Trainable: False\n",
      "Layer: trunk.blocks.6.attn.proj.weight | Trainable: False\n",
      "Layer: trunk.blocks.6.attn.proj.bias | Trainable: False\n",
      "Layer: trunk.blocks.6.norm2.weight | Trainable: False\n",
      "Layer: trunk.blocks.6.norm2.bias | Trainable: False\n",
      "Layer: trunk.blocks.6.mlp.fc1.weight | Trainable: False\n",
      "Layer: trunk.blocks.6.mlp.fc1.bias | Trainable: False\n",
      "Layer: trunk.blocks.6.mlp.fc2.weight | Trainable: False\n",
      "Layer: trunk.blocks.6.mlp.fc2.bias | Trainable: False\n",
      "Layer: trunk.blocks.7.norm1.weight | Trainable: False\n",
      "Layer: trunk.blocks.7.norm1.bias | Trainable: False\n",
      "Layer: trunk.blocks.7.attn.qkv.weight | Trainable: False\n",
      "Layer: trunk.blocks.7.attn.qkv.bias | Trainable: False\n",
      "Layer: trunk.blocks.7.attn.proj.weight | Trainable: False\n",
      "Layer: trunk.blocks.7.attn.proj.bias | Trainable: False\n",
      "Layer: trunk.blocks.7.norm2.weight | Trainable: False\n",
      "Layer: trunk.blocks.7.norm2.bias | Trainable: False\n",
      "Layer: trunk.blocks.7.mlp.fc1.weight | Trainable: False\n",
      "Layer: trunk.blocks.7.mlp.fc1.bias | Trainable: False\n",
      "Layer: trunk.blocks.7.mlp.fc2.weight | Trainable: False\n",
      "Layer: trunk.blocks.7.mlp.fc2.bias | Trainable: False\n",
      "Layer: trunk.blocks.8.norm1.weight | Trainable: False\n",
      "Layer: trunk.blocks.8.norm1.bias | Trainable: False\n",
      "Layer: trunk.blocks.8.attn.qkv.weight | Trainable: False\n",
      "Layer: trunk.blocks.8.attn.qkv.bias | Trainable: False\n",
      "Layer: trunk.blocks.8.attn.proj.weight | Trainable: False\n",
      "Layer: trunk.blocks.8.attn.proj.bias | Trainable: False\n",
      "Layer: trunk.blocks.8.norm2.weight | Trainable: False\n",
      "Layer: trunk.blocks.8.norm2.bias | Trainable: False\n",
      "Layer: trunk.blocks.8.mlp.fc1.weight | Trainable: False\n",
      "Layer: trunk.blocks.8.mlp.fc1.bias | Trainable: False\n",
      "Layer: trunk.blocks.8.mlp.fc2.weight | Trainable: False\n",
      "Layer: trunk.blocks.8.mlp.fc2.bias | Trainable: False\n",
      "Layer: trunk.blocks.9.norm1.weight | Trainable: True\n",
      "Layer: trunk.blocks.9.norm1.bias | Trainable: True\n",
      "Layer: trunk.blocks.9.attn.qkv.weight | Trainable: True\n",
      "Layer: trunk.blocks.9.attn.qkv.bias | Trainable: True\n",
      "Layer: trunk.blocks.9.attn.proj.weight | Trainable: True\n",
      "Layer: trunk.blocks.9.attn.proj.bias | Trainable: True\n",
      "Layer: trunk.blocks.9.norm2.weight | Trainable: True\n",
      "Layer: trunk.blocks.9.norm2.bias | Trainable: True\n",
      "Layer: trunk.blocks.9.mlp.fc1.weight | Trainable: True\n",
      "Layer: trunk.blocks.9.mlp.fc1.bias | Trainable: True\n",
      "Layer: trunk.blocks.9.mlp.fc2.weight | Trainable: True\n",
      "Layer: trunk.blocks.9.mlp.fc2.bias | Trainable: True\n",
      "Layer: trunk.blocks.10.norm1.weight | Trainable: True\n",
      "Layer: trunk.blocks.10.norm1.bias | Trainable: True\n",
      "Layer: trunk.blocks.10.attn.qkv.weight | Trainable: True\n",
      "Layer: trunk.blocks.10.attn.qkv.bias | Trainable: True\n",
      "Layer: trunk.blocks.10.attn.proj.weight | Trainable: True\n",
      "Layer: trunk.blocks.10.attn.proj.bias | Trainable: True\n",
      "Layer: trunk.blocks.10.norm2.weight | Trainable: True\n",
      "Layer: trunk.blocks.10.norm2.bias | Trainable: True\n",
      "Layer: trunk.blocks.10.mlp.fc1.weight | Trainable: True\n",
      "Layer: trunk.blocks.10.mlp.fc1.bias | Trainable: True\n",
      "Layer: trunk.blocks.10.mlp.fc2.weight | Trainable: True\n",
      "Layer: trunk.blocks.10.mlp.fc2.bias | Trainable: True\n",
      "Layer: trunk.blocks.11.norm1.weight | Trainable: True\n",
      "Layer: trunk.blocks.11.norm1.bias | Trainable: True\n",
      "Layer: trunk.blocks.11.attn.qkv.weight | Trainable: True\n",
      "Layer: trunk.blocks.11.attn.qkv.bias | Trainable: True\n",
      "Layer: trunk.blocks.11.attn.proj.weight | Trainable: True\n",
      "Layer: trunk.blocks.11.attn.proj.bias | Trainable: True\n",
      "Layer: trunk.blocks.11.norm2.weight | Trainable: True\n",
      "Layer: trunk.blocks.11.norm2.bias | Trainable: True\n",
      "Layer: trunk.blocks.11.mlp.fc1.weight | Trainable: True\n",
      "Layer: trunk.blocks.11.mlp.fc1.bias | Trainable: True\n",
      "Layer: trunk.blocks.11.mlp.fc2.weight | Trainable: True\n",
      "Layer: trunk.blocks.11.mlp.fc2.bias | Trainable: True\n",
      "Layer: trunk.norm.weight | Trainable: False\n",
      "Layer: trunk.norm.bias | Trainable: False\n",
      "Layer: attn_pool_contrast.query | Trainable: False\n",
      "Layer: attn_pool_contrast.attn.q_proj_weight | Trainable: False\n",
      "Layer: attn_pool_contrast.attn.k_proj_weight | Trainable: False\n",
      "Layer: attn_pool_contrast.attn.v_proj_weight | Trainable: False\n",
      "Layer: attn_pool_contrast.attn.in_proj_bias | Trainable: False\n",
      "Layer: attn_pool_contrast.attn.out_proj.weight | Trainable: False\n",
      "Layer: attn_pool_contrast.attn.out_proj.bias | Trainable: False\n",
      "Layer: attn_pool_contrast.ln_q.weight | Trainable: False\n",
      "Layer: attn_pool_contrast.ln_q.bias | Trainable: False\n",
      "Layer: attn_pool_contrast.ln_k.weight | Trainable: False\n",
      "Layer: attn_pool_contrast.ln_k.bias | Trainable: False\n",
      "Layer: ln_contrast.weight | Trainable: False\n",
      "Layer: ln_contrast.bias | Trainable: False\n",
      "Layer: attn_pool_caption.query | Trainable: False\n",
      "Layer: attn_pool_caption.attn.in_proj_weight | Trainable: False\n",
      "Layer: attn_pool_caption.attn.in_proj_bias | Trainable: False\n",
      "Layer: attn_pool_caption.attn.out_proj.weight | Trainable: False\n",
      "Layer: attn_pool_caption.attn.out_proj.bias | Trainable: False\n",
      "Layer: attn_pool_caption.ln_q.weight | Trainable: False\n",
      "Layer: attn_pool_caption.ln_q.bias | Trainable: False\n",
      "Layer: attn_pool_caption.ln_k.weight | Trainable: False\n",
      "Layer: attn_pool_caption.ln_k.bias | Trainable: False\n",
      "Layer: ln_caption.weight | Trainable: False\n",
      "Layer: ln_caption.bias | Trainable: False\n"
     ]
    }
   ],
   "source": [
    "for name, param in model_vit.named_parameters():\n",
    "    print(f\"Layer: {name} | Trainable: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 3-layer MLP\n",
    "st_raw_count_dims = 280\n",
    "output_dims = 512\n",
    "\n",
    "model_mlp = torch.nn.Sequential(\n",
    "    torch.nn.Linear(st_raw_count_dims, output_dims*2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(output_dims*2, output_dims*2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(output_dims*2, output_dims)\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytroch dataloader for images only\n",
    "import h5py\n",
    "import scanpy as sc\n",
    "\n",
    "class ImageExpressionDataset(Dataset):\n",
    "    def __init__(self, cases, image_dir, expression_dir, selected_genes=None, transform=None):\n",
    "        self.cases = cases\n",
    "        self.image_dir = image_dir\n",
    "        self.expression_dir = expression_dir\n",
    "        self.transform = transform\n",
    "        self.selected_genes = selected_genes\n",
    "        \n",
    "        self.image_size = 224\n",
    "        \n",
    "        # TODO: This is not the optimal way to load the data, \n",
    "        # as it loads all the data in memory, and is very slow at start up\n",
    "\n",
    "        # From dataset, read what cases to load\n",
    "        # Load cases as anndata\n",
    "        # Filter genes that are included from list\n",
    "        self.data_df = self.load_data()\n",
    "        print(f'Loaded {len(self.data_df)} patches with expression data')\n",
    "\n",
    "\n",
    "    def load_data(self):\n",
    "        # For each case, load the expression data and the patches\n",
    "        dfs = []\n",
    "        for case in self.cases:\n",
    "            # Load the patches\n",
    "            df_patches = self.load_patches(case)\n",
    "            # Load the expression\n",
    "            adata = self.load_expressions(case)\n",
    "            # Merge the data\n",
    "            df_patches.loc[:, 'expression'] = list(adata[df_patches.barcode, :].X)\n",
    "            \n",
    "            dfs.append(df_patches)\n",
    "\n",
    "        return pd.concat(dfs)\n",
    "\n",
    "    def load_patches(self, case):\n",
    "        # Open the file in read mode\n",
    "        with h5py.File(f'{self.image_dir}/{case}.h5', 'r') as file:\n",
    "            # Get the data\n",
    "            imgs = list(file['img'])\n",
    "            barcodes = list(file['barcode'])\n",
    "            coords = list(file['coords'])\n",
    "\n",
    "        barcodes = [b[0].decode('utf-8') for b in barcodes]\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                'barcode': barcodes,\n",
    "                'coord': coords,\n",
    "                'img': [Image.fromarray(im) for im in imgs]\n",
    "            }\n",
    "        )\n",
    "        return df\n",
    "\n",
    "\n",
    "    def load_expressions(self, case):\n",
    "       \n",
    "        adata = sc.read_h5ad(f'{self.expression_dir}/{case}.h5ad')\n",
    "        adata.obs['batch'] = case  # Add filename as batch key\n",
    "\n",
    "        # Filter to include only selected genes\n",
    "        if self.selected_genes is not None:\n",
    "            adata = adata[:, self.selected_genes]\n",
    "\n",
    "        # Normalize and log transform\n",
    "        # TODO: Should this be done for the whole dataset or per case?\n",
    "        sc.pp.normalize_total(adata)\n",
    "        # Log transformation\n",
    "        sc.pp.log1p(adata)\n",
    "        return adata\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df.index)\n",
    "   \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data_df.iloc[index]\n",
    "        \n",
    "        image = row.img\n",
    "        expression = row.expression\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = transforms.ToTensor()(image)\n",
    "\n",
    "        return image, expression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=448, interpolation=bicubic, max_size=None, antialias=True)\n",
       "    CenterCrop(size=(448, 448))\n",
       "    <function _convert_to_rgb at 0x7ff3ad524c10>\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hu-eki/miniconda3/envs/conch/lib/python3.10/site-packages/scanpy/preprocessing/_normalization.py:207: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n",
      "/home/hu-eki/miniconda3/envs/conch/lib/python3.10/site-packages/scanpy/preprocessing/_normalization.py:207: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n",
      "/home/hu-eki/miniconda3/envs/conch/lib/python3.10/site-packages/scanpy/preprocessing/_normalization.py:234: UserWarning: Some cells have zero counts\n",
      "  warn(UserWarning(\"Some cells have zero counts\"))\n",
      "/home/hu-eki/miniconda3/envs/conch/lib/python3.10/site-packages/scanpy/preprocessing/_normalization.py:207: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n",
      "/home/hu-eki/miniconda3/envs/conch/lib/python3.10/site-packages/scanpy/preprocessing/_normalization.py:234: UserWarning: Some cells have zero counts\n",
      "  warn(UserWarning(\"Some cells have zero counts\"))\n",
      "/home/hu-eki/miniconda3/envs/conch/lib/python3.10/site-packages/scanpy/preprocessing/_normalization.py:207: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n",
      "/home/hu-eki/miniconda3/envs/conch/lib/python3.10/site-packages/scanpy/preprocessing/_normalization.py:234: UserWarning: Some cells have zero counts\n",
      "  warn(UserWarning(\"Some cells have zero counts\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 41893 patches with expression data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hu-eki/miniconda3/envs/conch/lib/python3.10/site-packages/scanpy/preprocessing/_normalization.py:207: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n",
      "/home/hu-eki/miniconda3/envs/conch/lib/python3.10/site-packages/scanpy/preprocessing/_normalization.py:234: UserWarning: Some cells have zero counts\n",
      "  warn(UserWarning(\"Some cells have zero counts\"))\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of your dataset\n",
    "import json\n",
    "aug = transforms.Compose([\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip()\n",
    "    #transforms.Resize((224, 224)),\n",
    "    #transforms.ToTensor(),\n",
    "    #transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711)) # from conch preprocess\n",
    "])\n",
    "\n",
    "aug.transforms.extend(preprocess.transforms)\n",
    "\n",
    "cases = ['TENX99', 'TENX96', 'TENX95', 'NCBI783', 'NCBI785']\n",
    "expression_dir = '/home/hu-eki/Projects/HEST/tutorials/hest_data/st'\n",
    "image_dir = '/home/hu-eki/Projects/HEST/tutorials/hest_data/patches'\n",
    "selected_genes = json.load(open('filtered_genes.json'))\n",
    "\n",
    "dataset = ImageExpressionDataset(cases=cases, image_dir=image_dir, expression_dir=expression_dir,\n",
    "                                 selected_genes=selected_genes, transform=aug)\n",
    "\n",
    "# # Create a PyTorch DataLoader\n",
    "batch_size = 32 # TODO: change to 1024\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 448, 448])\n",
      "torch.Size([32, 280])\n"
     ]
    }
   ],
   "source": [
    "# Test dataloader\n",
    "for image, expression in data_loader:\n",
    "    print(image.shape)\n",
    "    print(expression.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add layerwise learning rate decay rate of 0.7 for ViT model\n",
    "# Define different learning rates for each layer\n",
    "lr_decay = 0.7\n",
    "lr_layer_3 = 1e-4\n",
    "lr_layer_2 = lr_layer_3 * lr_decay\n",
    "lr_layer_1 = lr_layer_2 * lr_decay\n",
    "\n",
    "# Create parameter groups with different learning rates\n",
    "param_groups = [\n",
    "    {'params': model_vit.trunk.blocks[-3].parameters(), 'lr': lr_layer_1},\n",
    "    {'params': model_vit.trunk.blocks[-2].parameters(), 'lr': lr_layer_2},\n",
    "    {'params': model_vit.trunk.blocks[-1].parameters(), 'lr': lr_layer_3}\n",
    "]\n",
    "\n",
    "# Pass the parameter groups to the optimizer\n",
    "optimizer_vit = torch.optim.Adam(param_groups)\n",
    "\n",
    "# Set up the optimizer to only optimize the paramters that are trainable\n",
    "# optimizer_vit = torch.optim.Adam(filter(lambda p: p.requires_grad, model_vit.parameters()), lr=0.0004)\n",
    "# MLP optimizer\n",
    "optimizer_mlp = torch.optim.Adam(model_mlp.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "scheduler_vit = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_vit, T_max=epochs)\n",
    "scheduler_mlp = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_mlp, T_max=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.02 # according to the paper\n",
    "# info-nce-pytorch\n",
    "infoloss = InfoNCE(temperature=temperature, negative_mode='unpaired')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=280, out_features=1024, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=1024, out_features=512, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_vit.cuda()\n",
    "model_mlp.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch: [1/50] Loss: 2.2398:   4%|▍         | 50/1310 [00:34<14:20,  1.46it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m model_mlp\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      6\u001b[0m total_loss, total_num, train_bar \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0\u001b[39m, tqdm(data_loader)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img, expr \u001b[38;5;129;01min\u001b[39;00m train_bar:\n\u001b[1;32m      8\u001b[0m     img, expr \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mcuda(non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), expr\u001b[38;5;241m.\u001b[39mcuda(non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m     out_1, _ \u001b[38;5;241m=\u001b[39m model_vit(img)\n",
      "File \u001b[0;32m~/miniconda3/envs/conch/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/conch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/conch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/conch/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/conch/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[7], line 87\u001b[0m, in \u001b[0;36mImageExpressionDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     84\u001b[0m expression \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mexpression\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     89\u001b[0m     image \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mToTensor()(image)\n",
      "File \u001b[0;32m~/miniconda3/envs/conch/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/miniconda3/envs/conch/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/conch/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/conch/lib/python3.10/site-packages/torchvision/transforms/transforms.py:1280\u001b[0m, in \u001b[0;36mColorJitter.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1278\u001b[0m         img \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39madjust_saturation(img, saturation_factor)\n\u001b[1;32m   1279\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m fn_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m hue_factor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1280\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjust_hue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/miniconda3/envs/conch/lib/python3.10/site-packages/torchvision/transforms/functional.py:968\u001b[0m, in \u001b[0;36madjust_hue\u001b[0;34m(img, hue_factor)\u001b[0m\n\u001b[1;32m    966\u001b[0m     _log_api_usage_once(adjust_hue)\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madjust_hue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhue_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39madjust_hue(img, hue_factor)\n",
      "File \u001b[0;32m~/miniconda3/envs/conch/lib/python3.10/site-packages/torchvision/transforms/_functional_pil.py:117\u001b[0m, in \u001b[0;36madjust_hue\u001b[0;34m(img, hue_factor)\u001b[0m\n\u001b[1;32m    113\u001b[0m np_h \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(hue_factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m    115\u001b[0m h \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(np_h, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 117\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHSV\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/miniconda3/envs/conch/lib/python3.10/site-packages/PIL/Image.py:1152\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     dither \u001b[38;5;241m=\u001b[39m Dither\u001b[38;5;241m.\u001b[39mFLOYDSTEINBERG\n\u001b[1;32m   1151\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1152\u001b[0m     im \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdither\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m   1154\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1155\u001b[0m         \u001b[38;5;66;03m# normalize source image and try again\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "\n",
    "    model_vit.train()\n",
    "    model_mlp.train()\n",
    "\n",
    "    total_loss, total_num, train_bar = 0.0, 0, tqdm(data_loader)\n",
    "    for img, expr in train_bar:\n",
    "        img, expr = img.cuda(non_blocking=True), expr.cuda(non_blocking=True)\n",
    "        out_1, _ = model_vit(img)\n",
    "        out_2 = model_mlp(expr)\n",
    "        \n",
    "        loss = infoloss(out_1, out_2)\n",
    "        optimizer_vit.zero_grad()\n",
    "        optimizer_mlp.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_vit.step()\n",
    "        optimizer_mlp.step()\n",
    "\n",
    "        total_num += batch_size\n",
    "        total_loss += loss.item() * batch_size\n",
    "        train_loss = total_loss / total_num\n",
    "        train_bar.set_description('Train Epoch: [{}/{}] Loss: {:.4f}'.format(epoch, epochs, train_loss))\n",
    "\n",
    "        \n",
    "    \n",
    "    scheduler_vit.step()\n",
    "    scheduler_mlp.step()\n",
    "    torch.save(model_vit.state_dict(), 'model_vit_50_preprocess.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 5\n",
    "for sample in range(1, samples+1):\n",
    "    torch.save(model_vit.state_dict(), f'model_vit_base_id_{sample}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hu-eki/Projects/CONCH/conch/open_clip_custom/factory.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=map_location)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'your_hugging_face_token'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, preprocess = create_model_from_pretrained(model_cfg, checkpoint_path, hf_auth_token=hf_auth_token)\n",
    "model_vit = model.visual\n",
    "torch.save(model_vit.state_dict(), f'model_vit_base_id_{9}.pth')\n",
    "preprocess\n",
    "hf_auth_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from conch.open_clip_custom import create_model_from_pretrained\n",
    "\n",
    "# Set up the configuration\n",
    "local_weights = False\n",
    "model_cfg = 'conch_ViT-B-16'\n",
    "\n",
    "if local_weights:\n",
    "    checkpoint_path = './checkpoints/CONCH/pytorch_model.bin'\n",
    "else:\n",
    "    checkpoint_path = 'hf_hub:MahmoodLab/conch'\n",
    "    hf_auth_token = os.getenv(\"HF_AUTH_TOKEN\")\n",
    "    if hf_auth_token is None:\n",
    "        raise ValueError(\"HF_AUTH_TOKEN environment variable not set\")\n",
    "\n",
    "# Load the model\n",
    "model, preprocess = create_model_from_pretrained(model_cfg, checkpoint_path, hf_auth_token=hf_auth_token)\n",
    "\n",
    "# We are only interested in the ViT part of the model\n",
    "model_vit = model.visual\n",
    "\n",
    "# Print the model to verify\n",
    "print(model_vit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
